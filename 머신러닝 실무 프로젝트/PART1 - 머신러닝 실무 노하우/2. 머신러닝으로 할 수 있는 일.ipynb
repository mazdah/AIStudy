{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af6b2a52-75c7-4219-9c98-52590857e89d",
   "metadata": {},
   "source": [
    "# 2.1 머신러닝 알고리즘 선택 방법\n",
    "\n",
    "- **분류**: 정답이 비연속적인 클래스(카테고리)이며, 정답과 입력 데이터의 조합을 학습하여 새로운 데이터의 클래스를 예측한다.\n",
    "- **회귀**: 정답이 수치이다. 정답과 입력 데이터의 조합을 학습하여 새로운 데이터에서 연속하는 값을 예측한다.\n",
    "- **클러스터링(군집화)**: 어떤 기준에 따라 데이터를 그룹으로 묶는다.\n",
    "- **차원 축소**: 시각화 또는 계산량 감소를 목적으로 고차원 데이터를 저차원 공간에 매핑한다.\n",
    "- **기타**\n",
    "    - 추천: 사용자가 좋아하는 아이템, 살펴보고 있는 아이템과 비슷한 아이템을 제시한다.\n",
    "    - 이상 탐지: 부정한 접근 등 평소와 다른 행동을 검출한다.\n",
    "    - 고빈도 패턴 마이닝: 데이터 안에서 발생 빈도가 높은 패턴을 추출한다.\n",
    "    - 강화 학습: 바둑이나 장기처럼 정답이 명확하지 않은 환경에서, 앞으로 취할 행동을 선택하는 방법을 학습한다.\n",
    "    \n",
    "    ---\n",
    "    \n",
    "- 알고리즘 선택 요령\n",
    "    ![알고리즘 선택 요령](https://scikit-learn.org/stable/_static/ml_map.png)\n",
    "    \n",
    "    ---\n",
    "    \n",
    "- 학습에 사용하는 데이터 수, 예측 대상이 비연속적인지(클래스), 정답 레이블이 존재하는지 등의 정보가 핵심"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eb18ed-8a3a-49e2-bb48-2157cfb39532",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9eca6e-c9e7-4e0f-b820-fd30567465a1",
   "metadata": {},
   "source": [
    "# 2.2 분류\n",
    "\n",
    "- **분류**<sup>classification</sup>란 지도 학습의 한 종류로 범주와 같은 비연속적인 값을 예측한다.\n",
    "- 클래스 수가 2개이면 이진 분류, 3개 이상이면 다중값 분류 또는 멀티클래스 분류<sup>multiclass classification</sup>라고 한다.\n",
    "- 분류 알고리즘 들\n",
    "    - **퍼셉트론**<sup>perceptron</sup>\n",
    "    - **로지스틱 회귀**<sup>logistic regression</sup>\n",
    "    - **서포트 벡터 머신**<sup>support vector machine</sup>(SVM)\n",
    "    - **신경망**<sup>neural network</sup>\n",
    "    - **k-최근접 이웃 알고리즘**<sup>k-Nearest Neighbor</sup>(k-NN)\n",
    "    - **결정 트리**<sup>decision tree</sup>\n",
    "    - **랜덤 포레스트**<sup>random forest</sup>\n",
    "    - **경사 부스팅 결정 트리**<sup>gradient boosted decision tree</sup>(GBDT)\n",
    "    \n",
    "- 퍼셉트론, 로지스틱 회귀, SVM, 신경망은 두 클래스의 경계면(초평면)에 대한 함수를 학습\n",
    "> 두 클래스를 분류하는 평면을 **결정 경계**<sup>decision boundary</sup>라고 한다.\n",
    "\n",
    "- k-NN은 최근접 이웃 알고리즘이라고도 하며, 학습 완료된 데이터와의 거리가 가까운 데이터를 기반으로 판단\n",
    "- 결정 트리, 랜덤 포레스트, GBDT는 트리 구조로 표현된 규칙의 집함을 학습\n",
    "- 대부분의 분류 문제는 **목적 함수**와 **결정 경계**를 이해하면 쉽게 차이를 이해할 수 있다.\n",
    "\n",
    "### 2.2.1 퍼셉트론\n",
    "\n",
    "- 입력 벡터와 학습한 가중치 벡터를 곱한 값을 합한 값이 0 이상일 때는 클래스1로, 0 미만일 때는 클래스2로 분류하는 간단한 알고리즘\n",
    "- 퍼셉트론을 여러 층 쌓으면 신경망이 된다.\n",
    "- 퍼셉트론의 특성\n",
    "    - 온라인 학습 방식을 취한다.\n",
    "    - 예측 성능은 보통이지만 학습이 빠르다.\n",
    "    - 과적합 되기 쉽다.\n",
    "    - 선형 분리 가능한 문제만 풀 수 있다.\n",
    "\n",
    "---\n",
    "![퍼셉트론](./images/perceptron.png)\n",
    "\n",
    "#### **퍼셉트론의 결정 경계**\n",
    "\n",
    "- 퍼셉트론은 비선형 분리를 할 수 없으므로 결정 경계가 항상 직선이다.\n",
    "\n",
    "#### **퍼셉트론의 구조**\n",
    "\n",
    "- 특징이 2차원인 간단한 예, 입력을 리스트 x, 가중치를 리스트 w로, 편향을 b(현재는 편의상 무시)로 표현\n",
    "\n",
    "> sum = b + w[0] * x[0] + w[1] * x[1]\n",
    "\n",
    "- 위 식의 코드는 아래와 같으며 sum의 값에 따라 클래스를 판단한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27ff7acf-7a28-42b4-a2c4-090bd16dec8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "w = np.array([2, 3])\n",
    "x = np.array([4, 2])\n",
    "sum = np.dot(w, x)\n",
    "sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6267a314-9a10-4902-a004-65248b83cc7c",
   "metadata": {},
   "source": [
    "- 예측 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc1147fb-a961-4b73-9016-07f15b03ba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 퍼셉트론을 이용한 예측\n",
    "def predict(w, x):\n",
    "    sum = np.dot(w, x)\n",
    "    \n",
    "    if sum > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44d15a4-c34f-4a25-bc34-b98560d36620",
   "metadata": {},
   "source": [
    "- 적절한 파라미터(w)를 알아내는 방법: 실젯값과 예측값이 얼마나 다른지를 나타내는 함수 사용(**손실 함수** 또는 **오차 함수**라 부름).\n",
    "> 오차의 제곱을 손실 함수로 사용하는 식: **손실 함수 = (실젯값 - 예축값)<sup>2</sup>**\n",
    "\n",
    "- 가중치 벡터를 w, 입력 벡터를 x, 정답 레이블을 t(1혹은 -1)라고 했을 때 퍼셉트론의 손실함수는 **max(0, -twx)(힌지 손실)**<sup>hinge loss</sup>\n",
    "- 퍼셉트론의 힌지 손실 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97896cd6-2475-47be-a2f2-bc3bb535e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def perceptron_hinge_loss(w, x, t):\n",
    "    loss = 0\n",
    "    \n",
    "    for (input, label) in zip(x, t):\n",
    "        v = label * np.dot(w, input)\n",
    "        loss += max(0, -v)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382498f7-b7c5-481c-964e-cebfb0b56eca",
   "metadata": {},
   "source": [
    "- 손실 함수를 일반화하여 **목적 함수**<sup>objective function</sup>(평가 함수)로 만든다.\n",
    "> 퍼셉트론의 목적 함수: **목적 함수 = 손실 함수의 모든 데이터의 합**\n",
    "\n",
    "- 목적 함수가 최소가 되는(최적 상태) 가중치 벡터 w를 구하는 과정이 **'모델 학습'**\n",
    "- 파라미터(가중치 벡터 w) 최적화에는 **확률적 경사하강법**<sup>stochastic gradient descent</sup>(SGD)를 많이 사용\n",
    "- 파라미터를 한 번에 어느 정도씩 수정할지를 결정하는 하이퍼파라미터가 **학습률**<sup>learning rate</sup>\n",
    "- 출력 값을 비선형 변환하는 함수는 **활성화 함수**<sup>activation function</sup>\n",
    "- 퍼셉트론의 예측값은 가중치 벡터와 입력 벡터의 곱셈합의 부호(양수/음수)로 결정된다. 이는 곱셈합을 **계단 함수**<sup>step function</sup>에 통과시킨 것과 같다.\n",
    "\n",
    "### 2.2.2 로지스틱 회귀\n",
    "\n",
    "- 이름은 회귀지만 분류 알고리즘\n",
    "- 간단하면서도 파라미터가 많지 않아 빠르게 예측 가능\n",
    "\n",
    "---\n",
    "![로지스틱회귀](./images/logistic.png)\n",
    "\n",
    "#### **로지스틱 회귀의 특성**\n",
    "\n",
    "- 출력과 별도로, 출력 값에 해당하는 클래스에 속할 '확률값'을 반환한다.\n",
    "- 온라인 학습과 배치 학습이 모두 가능하다.\n",
    "- 예측 성능은 보통이며 학습 및 추론 속도가 빠르다.\n",
    "- 과적합을 방지하는 규제항이 추가되어 있다.\n",
    "\n",
    "#### **로지스틱 회귀의 결정 경계**\n",
    "\n",
    "- 결정 경계가 직선이다(선형 분리만 가능)\n",
    "\n",
    "#### **로지스틱 회귀의 구조**\n",
    "\n",
    "- 활성화 함수가 시그모이드 함수<sup>sigmoid function</sup>(혹은 로지스틱 시그모이드 함수<sup>logistic sigmoid function</sup>)\n",
    "- 손실 함수가 교차 엔트로피 오차 함수<sup>cross-entropy error function</sup>이다.\n",
    "- 규제항(정규화항)<sup>regularization term</sup>이 추가되어 과적합을 방지할 수 있다. \n",
    "- 시그모이드 함수 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "745600c4-6a9c-43e0-a514-b59ac3f2e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a42a1-0c87-454a-8030-6a46bdab6e63",
   "metadata": {},
   "source": [
    "- 출력 `y = sigmoid(np.dot(w, x))`로 나타낼 수 있다.\n",
    "- 교차 엔트로피 오차 함수의 식\n",
    "> $E = - \\displaystyle\\sum_{n=1}^{N}{t_n}\\log{y_n} + (1 - t_n)\\log{1-y_n}$  \n",
    "N: 데이터 수, y: 출력, t: 정답 레이블(정답은 1, 틀리면 0), 로그의 밑은 자연상수\n",
    "- 교차 엔트로피 함수 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5295377a-4cdb-48ae-8a83-547dd172af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t, eps=1e-15):\n",
    "    y_clipped = np.clip(y, eps, 1-eps) # 데이터가 1건이라면 max(min(y, 1 - eps), eps)와 동일하다.\n",
    "    return -1 * (sum(t * np.log(y_clipped) + (1-t) * np.log(1 - y_clipped)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2212a7-3e8b-4766-8aed-2ab05f2a204f",
   "metadata": {},
   "source": [
    "- 로그 값이 음의 무한대가 되는 일을 피하기 위해 y 값이 0 또는 1이 되지 않도록 eps라는 작은 값을 더해준다.\n",
    "- **정규화**<sup>regularization</sup>는 패널티를 부여하여 결정 경계를 매끄럽게 하며, 단순한 모델을 유지하도록 보정한다.\n",
    "- 결국 과적합을 방지하도록 한다.\n",
    "- 정규화항이 포함된 목적 함수\n",
    "> 목적 함수 = 모든 데이터에 대한 손실 함수 값의 합 + 정규화항\n",
    "- 확률적 경사하강법으로 최적화\n",
    "> L2 정규화: 가중치의 제곱의 합을 정규화항으로 사용. 큰 가중치에 페널티가 부여된다. 릿지 회귀에 사용  \n",
    "목적 함수 = 모든 데이터에 대한 손실 함수 값의 합 + $\\lambda\\displaystyle\\sum_{i=1}^{m}{w_i^2}$  \n",
    "L1 정규화: 가중치의 절댓값의 합을 정규화항으로 사용. 가중치가 0으로 제거되는 경우가 많아진다. 라쏘 회귀에 사용  \n",
    "목적 함수 = 모든 데이터에 대한 손실 함수 값의 합 + $\\lambda\\displaystyle\\sum_{i=1}^{m}\\vert{w_i}\\vert$  \n",
    "일래스틱넷은 두 규제를 모두 사용\n",
    "\n",
    "### 2.2.3 서포트 벡터 머신<sup>support vector machine</sup> (SVM)\n",
    "\n",
    "- 분류에서 매우 많이 사용\n",
    "- **선형 분리 불가능한 문제에도 적용 가능**\n",
    "- 다양한 알고리즘과 라이브러리 존재\n",
    "- 선형 커널 이외의 SVM은 데이터 수에 비례하여 계산 시간이 늘어나 대규모 데이터에는 잘 사용하지 않는다.\n",
    "\n",
    "#### **SVM의 특성**\n",
    "- **마진 최대화**를 통해 매끈한 초평면을 학습할 수 있다.\n",
    "- **커널**<sup>kernel</sup>이라는 방법을 사용하여 비선형 데이터를 분리할 수 있다.\n",
    "- 선형 커널로는 차원 수가 높은 희소<sup>sparse</sup> 데이터도 학습할 수 있다.\n",
    "- 배치 학습과 온라인 학습에 모두 적용할 수 있다.\n",
    "\n",
    "#### **SVM의 결정 경계**\n",
    "\n",
    "- 선형 커널은 직선으로 분리, RBF 커널은 비선형으로 분리\n",
    "\n",
    "#### **SVM의 구조**\n",
    "\n",
    "- 손실 함수는 힌지 손실을 사용(퍼셉트론과는 가로축과의 교점이 달라 결정 경계에 마진이 생긴다).\n",
    "- 마진을 최대화하면서 규제화항과 비슷한 과적합 억제 효과를 얻는다.\n",
    "- 커널 기법 사용\n",
    "> 커널 기법 : 선형 분리 불가능한 데이터에 커널 함수를 적용하여 데이터의 차원 수를 늘려 선형 분리 가능한 형태로 바꾸는 방법  \n",
    "선형 커널<sup>linear kernel</sup>, 다항식 커널<sup>polynomial kernel</sup>, RBF 커널<sup>radial basis function kernel</sup>(동적 기저함수 커널) 등이 있다.  \n",
    "선형 커널은 속도가 빨라 텍스트 같은 고차원 희소 벡터(**입력 벡터의 요소 대부분이 0인 벡터**)에 사용  \n",
    "RBF 커널은 이미지나 음성신호 같은 조밀한 데이터에 주로 사용\n",
    "---\n",
    "![커널 기법](./images/kernel.png)\n",
    "\n",
    "### 2.2.4 신경망<sup>neural network</sup>\n",
    "\n",
    "- 다층 퍼셉트론이라고도 부름\n",
    "- 퍼셉트론을 하나의 노드로 하여 계층적으로 쌓아놓은 구조\n",
    "---\n",
    "![신경망](./images/nn.png)\n",
    "\n",
    "\n",
    "#### **신경망의 특성**\n",
    "\n",
    "- 비선형 데이터를 분리할 수 있다.\n",
    "- 학습 시간이 오래 걸린다.\n",
    "- 파라미터 수가 많으므로 과적합을 일으키기 쉽다.\n",
    "- 가중치의 초깃값에 민감하며, 국소 최적해<sup>local optimal solution</sup>에 빠지기 쉽다.\n",
    "- 데이터가 많이 필요하다.\n",
    "\n",
    "#### **신경망의 결정 경게**\n",
    "\n",
    "- 직선이 아닌 결정 경계를 가질 수 있다.\n",
    "\n",
    "#### **신경망의 구조**\n",
    "\n",
    "- 입력층, 은닉층(중간층), 출력층 순서로 입력과 가중치의 곱셈합을 구한다.\n",
    "- 출력층에는 분류하려는 클래스의 수만큼 노드를 배치한다.\n",
    "- 출력층에서 계산한 값을 **softmax 함수**로 정규화한 값을 확률로 사용하는 경우가 많다.\n",
    "- 은닉층의 노드 수는 입력층과 출력층의 노드 수에 맞춰 적절히 설정\n",
    "- 활성화 함수는 **ReLU<sup>rectified linear unit</sup> 함수**가 많이 쓰인다.\n",
    "- 피드포워드 신셩망 학습에는 **오차 역전파**<sup>backpropagation</sup> 방법을 쓴다.\n",
    "- 신경망 중간층을 수를 늘리다보면 제대로 학습되지 않는 문제가 생기는데 이를 보완한 것이 딥러닝이다.\n",
    "\n",
    "### 2.5.5 k-최근점 이웃<sup>k-nearest neighbor</sup>(k-NN)\n",
    "\n",
    "- k-최근접 이웃은 새로운 데이터의 클래스를 다수결로 정한다.\n",
    "- 진행 과정\n",
    "    - 이미 학습된 데이터 중 새로 입력된 데이터와 거리가 가장 가까운 k 개를 선택\n",
    "    - 이 k개 중 가장 많은 데이터가 속한 클래스를 찾음\n",
    "    - 새로운 데이터를 이 클래스로 분류\n",
    "\n",
    "#### **k-NN의 특성**\n",
    "\n",
    "- 데이터를 하나씩 순차적으로 학습한다.\n",
    "- 기본적으로 모든 데이터와의 거리를 게산해야 하므로 예측 시간이 걸린다.\n",
    "- k값에 따라 편차는 있지만 예측 성능은 괜찮은 편이다.\n",
    "- 스케일 차이가 클수록 성능이 떨어지므로 정규화<sup>nomalization</sup>로 스케일 조정 필요\n",
    "\n",
    "#### **k-NN의 결정 경계**\n",
    "\n",
    "- k값 설정에 따라 결정 경계가 좌우됨(k값이 커질수록 결정 경계는 매끈해지나 처리 시간이 길어진다)\n",
    "\n",
    "#### **k-NN의 구조**\n",
    "\n",
    "- k는 예측 클래스를 결정하는 총 투표 수\n",
    "- k값은 교차검증을 통해 결정한다.\n",
    "- 두 데이터 사이의 거리는 유클리드 거리<sup>Euclidean distance</sup>다.\n",
    "- 데이터가 분포하는 방향을 고려하는 마할라노비스 거리<sup>Mahalanobis distance</sup>을 사용하기도 한다.\n",
    "- 높은 차원의 희소 데이터에서는 성능이 떨어져 차원 축소를 통한 개선이 필요하다.\n",
    "- 유클리드 거리를 구하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78d17b9b-9e30-4786-925c-e9fdd2b44a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(sum(x-y) ** 2 for (x, y) in zip(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f15767-1f4d-409c-be66-1ca9eae4c632",
   "metadata": {},
   "source": [
    "### 2.2.6 결정 트리, 랜덤 포레스트, GBDT\n",
    "\n",
    "- 결정트리는 트리 형태 알고리즘의 대표 주자\n",
    "---\n",
    "![결정 트리](./images/tree.png)\n",
    "\n",
    "#### **결정 트리의 특성**\n",
    "\n",
    "- 학습한 모델을 사람이 해석하기 쉽다.\n",
    "- 입력 데이터를 정규화할 필요가 없다.\n",
    "- 범주형 변수나 데이터의 누락값이 있어도 내부에서 처리해준다.\n",
    "- 특정 조건이 맞으면 과적합을 일으키는 경향이 있다(트리가 깊어질수록 또는 특징 수가 많을수록 과적합이 일어나기 쉽다).\n",
    "- 비선형 문제에는 적용할 수 있지만 선형 분리 문제는 잘 풀지 못한다.\n",
    "- 데이터 분포가 특정 클래스에 쏠려 있으면 잘 풀지 못한다.\n",
    "- 데이터의 작은 변화에도 결과가 크게 바귀기 쉽다.\n",
    "- 예측 성능은 보통이다.\n",
    "- 배치 학습으로만 학습할 수 있다.\n",
    "\n",
    "> 특정한 분류 결과에 이르게 되는 조건을 알아야 할 때 유용하다.  \n",
    "선형 분리 가능한 문제의 경우 트리의 깊이가 깊어질수록 데이터가 적어져 과적합되기 쉽다.  \n",
    "과적합 문제는 트리의 깊이를 줄이는 가지치기<sup>pruning</sup>으로 어느 정도 방지 가능하다.\n",
    "\n",
    "#### **결정 트리의 결정 경계**\n",
    "\n",
    "- 영역을 반복해서 나눠가는 과정에서 정해지기 때문에 직선 형태를 취하지 않는다.\n",
    "\n",
    "#### **결정 트리의 구조**\n",
    "\n",
    "- 학습 데이터로부터 조건식을 만들고, 예측할 때는 트리의 루트<sup>root node</sup>부터 순서대로 조건 분기를 타면서 리프<sup>leaf node</sup>에 도달하면 예측 결과를 낸다.\n",
    "- **불순도**<sup>impurity</sup>를 기준으로 가능한 한 같은 클래스끼리 모이도록 조건 분기를 학습한다.\n",
    "- 정보 획득<sup>information gain</sup>이나 지니 계수<sup>Gini coefficient</sup> 등의 값을 불순도로 사용한다.\n",
    "\n",
    "#### **결정 트리로부터 파생된 알고리즘**\n",
    "\n",
    "- 랜덤 포레스트<sup>random forest</sup>\n",
    "    - 이용할 샘플을 무작위로 선택(부트스트랩 샘플링 알고리즘)하고 이용할 특징량을 무작위로 선택하여 여러 트리를 만든다.\n",
    "    - 몇회귀에서는 각 결정 트리의 평균 결과를 내고 분류에서는 다수결로 선택한다.\n",
    "    - 각각의 트리는 독립적으로 학습(병렬 학습 가능)\n",
    "    - 가지치기를 하지 않아 트리의 수, 깊이, 조정할 파라미터 수가 적어 과적합 되기 쉽다.\n",
    "    - 결정 트리보다 성능이 좋고 파라미터 수가 적어 튜닝도 비교적 간단하다.\n",
    "    - 결정 경계는 결정 트리와 비슷하다.\n",
    "- GBDT<sup>gradient boosted decision tree</sup>\n",
    "    - 표본추출한 데이터를 이용해 순차적으로 얕은 트리를 학습해가는 경사 부스팅<sup>gradient boosting</sup>을 사용\n",
    "    - 예측값과 실젯값의 오차를 목표 변수로 삼음\n",
    "    - 여러 개의 학습기로 학습\n",
    "    - 순차적 학습으로 시간이 오래 걸림\n",
    "    - 파라미터 수가 많아 튜닝이 쉽지 않음\n",
    "    - 랜덤 포레스트보다 더 뛰어난 예측 성능\n",
    "    - XGBoost나 LightGBM 등의 라이브러리를 통해 대규모 데이터도 쉽고 빠르게 처리 가능\n",
    "    \n",
    "> 랜덤 포레스트나 GBDT처럼 여러 학습 결과를 조합하는 기법을 **앙상블 학습**<sup>ensemble learning</sup>이라 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e067ed29-0f65-4a02-a3b8-20b1ca76071f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a451d890-74fa-4cd4-99c1-59b3bc5830eb",
   "metadata": {},
   "source": [
    "# 2.3 회귀\n",
    "\n",
    "- 지도 학습\n",
    "- 입력 데이터로부터 연속값을 예측\n",
    "- 알고리즘\n",
    "    - 선형 회귀<sup>linear regression</sup>: 데이터를 직선으로 근사 \n",
    "    - 다항식 회귀<sup>polynomial regression</sup>: 데이터를 곡선으로 근사\n",
    "    - 릿지 회귀<sup>ridge regression</sup>: 학습하 가중치의 제곱을 규제항으로 사용(L2 Norm)\n",
    "    - 라쏘 회귀<sup>LASSO regression</sup>: 학습한 가중치의 절댓값을 규제항으로 사용(L1 Norm) \n",
    "    - 일래스틱 넷: 선형 회귀에 두 가지 규제항을 추가한 것. 라쏘 회귀 처럼 L1 규제를 이용하여 일부 가중치를 0으로 만듦\n",
    "    - 회귀 트리<sup>regression tree</sup>: 결정 트리에 기초한 회귀 기법. 비선형 테이터를 근사\n",
    "    - SVR<sup>support vector regression</sup>: SVM에 기초한 회귀 기법. 비선형 데이터를 근사\n",
    "    \n",
    "### 2.3.1 선형 회귀의 원리\n",
    "\n",
    "- 퍼셉트론에서 출력을 이진값으로 만드는 부분을 빼고 수치를 직접 출력하게 만든 구조\n",
    "---\n",
    "![선형 회귀](./images/regression.png)\n",
    "\n",
    "- 선형 회귀의 목적 함수\n",
    "> 목적함수 = 모든 데이터에 대한 손실 함수의 값의 합\n",
    "- 선형 회귀와 다항식 회귀 모델의 학습 결과는 각 변수를 합할 때 곱해지는 가중치이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf0d9f2-79b4-43fb-9897-abcc809c60b0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0061dbe-7df6-45de-bfae-bff77383f9d9",
   "metadata": {},
   "source": [
    "# 2.4 군집화와 차원 축소\n",
    "\n",
    "### 2.4.1 군집화<sup>clustering</sup>\n",
    "\n",
    "- 비지도 학습\n",
    "- 주로 데이터의 경향성을 파악하는 데 사용\n",
    "- 계층적 군집화<sup>hierarchical clustering</sup>, k-평균<sup>k-means</sup> 기법 등이 있다.\n",
    "\n",
    "### 2.4.2 차원 축소<sup>demention reduction</sup>\n",
    "\n",
    "- 정보를 가능한 한 온전히 보존하면서 고차원 데이터를 저차원 데이터로 변환하는 기법\n",
    "- 데이터를 시각화 하는 목적\n",
    "- 희소한<sup>sparse</sup> 데이터를 빽빽한<sup>dense</sup> 데이터로 변환하여 압축하는 용도\n",
    "- 차원 축소를 거친 데이터를 지도학습의 정답 데이터로 사용하는 경우도 있음\n",
    "- **주성분 분석**<sup>principal component analysis</sup>(PCA), t-SNE 등의 기법이 있음\n",
    "- t-SNE는 시각화에 많이 사용되면 PCA보다 데이터 관계성을 파악하기 쉬움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aca7fb-01a1-4339-9000-f293ded19480",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c00cb27-2702-4aac-8f51-82533e8b8806",
   "metadata": {},
   "source": [
    "# 2.5 그 외\n",
    "\n",
    "- 머신러닝으로 수행하는 작업 유형들\n",
    "    - 추천\n",
    "    - 이상 탐지\n",
    "    - 패턴 마이닝\n",
    "    - 강화 학습\n",
    "    \n",
    "### 2.5.1 추천<sup>recommendation</sup>\n",
    "\n",
    "- 사용자가 흥미 있어 할 만한 것이나 관심을 표현한 것과 비슷한 것을 미리 제시\n",
    "\n",
    "### 2.5.2 이상 탐지<sup>anormaly detection</sup>\n",
    "\n",
    "- 신용카드 부정사용, 서비스 거부 공격 등 이상 상태를 탐지\n",
    "- **이상값 탐지**<sup>outlier detection</sup>라고도 함\n",
    "- 데이터 분포가 극단적으로 치우치는 특성(이상치의 빈도는 매우 낮음)으로 비지도 학습 기법이 많이 사용된다.\n",
    "\n",
    "### 2.5.3 고빈도 패턴 마이닝<sup>frequent pattern mining</sup>\n",
    "\n",
    "- 짧게 패턴 마이닝이라고도 함\n",
    "- 데이터에서 매우 자주 발견되는 패턴을 추출하는 기법\n",
    "- **연관 법칙**<sup>association rule</sup>이 유명\n",
    "- 시계열 분석에는 **ARMA(자기회귀 이동평균 모델)** 알고리즘이 많이 사용됨\n",
    "\n",
    "### 2.5.4 강화 학습<sup>reinforcement learning</sup>\n",
    "\n",
    "- 경험과 시행착오를 거쳐 목적에 다다르기 위해 각 상황에서 취해야 할 최적의 행동 정책을 학습하는 기법\n",
    "- 자율주행, 게임 인공지능이 주 활용 분야"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a54595-9f36-4089-aeb3-e580ea721d39",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d896bb9-2960-407c-99c5-1bce9e570104",
   "metadata": {},
   "source": [
    "# 2.6 정리\n",
    "\n",
    "> 데이터의 경향을 살피며 다양한 알고리즘을 시도해보는 과정이 필요!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
