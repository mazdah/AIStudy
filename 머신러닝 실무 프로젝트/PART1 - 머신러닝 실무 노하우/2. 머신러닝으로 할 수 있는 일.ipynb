{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af6b2a52-75c7-4219-9c98-52590857e89d",
   "metadata": {},
   "source": [
    "# 2.1 머신러닝 알고리즘 선택 방법\n",
    "\n",
    "- **분류**: 정답이 비연속적인 클래스(카테고리)이며, 정답과 입력 데이터의 조합을 학습하여 새로운 데이터의 클래스를 예측한다.\n",
    "- **회귀**: 정답이 수치이다. 정답과 입력 데이터의 조합을 학습하여 새로운 데이터에서 연속하는 값을 예측한다.\n",
    "- **군집화**: 어떤 기준에 따라 데이터를 그룹으로 묶는다.\n",
    "- **차원 축소**: 시각화 또는 계산량 절감을 목적으로 고차원 데이터를 저차원 공간에 매핑한다.\n",
    "- **그 외**\n",
    "    - 추천: 사용자가 선호할 만한 품목, 혹은 여람한 것과 비슷한 품목을 제시한다.\n",
    "    - 이상 탐지: 수상한 접근 등 평소와 다른 행동을 검출한다.\n",
    "    - 고빈도 패턴 마이닝: 데이터 안에서 발생 빈도가 높은 패턴을 추출한다.\n",
    "    - 강화 학습: 바둑이나 장기처럼 정답이 불명확한 환경에서, 앞으로 취할 행동을 선택하는 방법을 학습한다.\n",
    "    \n",
    "    ---\n",
    "    \n",
    "- 알고리즘 선택 요령\n",
    "    ![알고리즘 선택 요령](https://scikit-learn.org/stable/_static/ml_map.png)\n",
    "    \n",
    "    ---\n",
    "    \n",
    "- 학습에 사용하는 데이터 수, 예측 대상이 비연속적인지(클래스), 정답 레이블이 존재하는지 등의 정보가 핵심"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eb18ed-8a3a-49e2-bb48-2157cfb39532",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9eca6e-c9e7-4e0f-b820-fd30567465a1",
   "metadata": {},
   "source": [
    "# 2.2 분류\n",
    "\n",
    "- 분류<sup>classification</sup>란 지도 학습의 한 종류로 카테고리와 같은 비연속적인 값을 예측한다.\n",
    "- 클래스 수가 2개이면 이진 분류, 3개 이상이면 다중클래스 분류라고 한다.\n",
    "- 분류 알고리즘 들\n",
    "    - 퍼셉트론\n",
    "    - 로지스틱 회귀\n",
    "    - 서포트 벡터 머신(SVM)\n",
    "    - 신경망\n",
    "    - k-최근접 이웃(k-NN)\n",
    "    - 결정 트리\n",
    "    - 랜덤 포레스트\n",
    "    - 경사 부스팅 결정 트리(GBDT)\n",
    "    \n",
    "- 퍼셉트론, 로지스틱 회귀, SVM, 신경망은 두 클래스의 경계면(초평면)에 대한 함수를 학습\n",
    "- k-최근접 이웃은 학습된 데이터와의 '거리'를 기준으로 판단\n",
    "- 결정 트리, 랜덤 포레스트, GBDT는 트리 구조로 표현된 규칙의 집함을 학습\n",
    "- 대부분의 분류 문제는 목적 함수와 결정 경계를 이해하면 쉽게 차이를 이해할 수 있다.\n",
    "\n",
    "### 2.2.1 퍼셉트론\n",
    "\n",
    "- 입력 벡터와 학습한 가중치 벡터를 곱한 값을 합한 값이 0 이상일 때는 클래스1로, 0 미만일 때는 클래스2로 분류하는 간단한 알고리즘\n",
    "- 퍼셉트론을 여러 층 쌓으면 신경망이 된다.\n",
    "- 퍼셉트론의 특성\n",
    "    - 온라인 학습 방식을 취한다.\n",
    "    - 예측 성능은 보통이지만 학습이 빠르다.\n",
    "    - 과적합 되기 쉽다.\n",
    "    - 선형 분리 가능한 문제만 풀 수 있다.\n",
    "\n",
    "---\n",
    "![퍼셉트론](./images/perceptron.png)\n",
    "\n",
    "#### **퍼셉트론의 결정 경계**\n",
    "\n",
    "- 퍼셉트론은 비선형 분리를 할 수 없으므로 결정 경계가 항상 직선이다.\n",
    "\n",
    "#### **퍼셉트론의 구조**\n",
    "\n",
    "- 특징이 2차원인 간단한 예, 입력을 리스트 x, 가중치를 리스트 w로, 편향을 b(현재는 편의상 무시)로 표현\n",
    "\n",
    "> sum = b + w[0] * x[0] + w[1] * x[1]\n",
    "\n",
    "- 위 식의 코드는 아래와 같으며 sum의 값에 따라 클래스를 판단한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27ff7acf-7a28-42b4-a2c4-090bd16dec8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "w = np.array([2, 3])\n",
    "x = np.array([4, 2])\n",
    "sum = np.dot(w, x)\n",
    "sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6267a314-9a10-4902-a004-65248b83cc7c",
   "metadata": {},
   "source": [
    "- 예측 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc1147fb-a961-4b73-9016-07f15b03ba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 퍼셉트론을 이용한 예측\n",
    "def predict(w, x):\n",
    "    sum = np.dot(w, x)\n",
    "    \n",
    "    if sum > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44d15a4-c34f-4a25-bc34-b98560d36620",
   "metadata": {},
   "source": [
    "- 적절한 파라미터(w)를 알아내는 방법: 실젯값과 예측값이 얼마나 다른지를 나타내는 함수 사용(**손실 함수** 또는 **오차 함수**라 부름).\n",
    "> 오차의 제곱을 손실 함수로 사용하는 식: **손실 함수 = (실젯값 - 예축값)<sup>2</sup>**\n",
    "\n",
    "- 가중치 벡터를 w, 입력 벡터를 x, 정답 레이블을 t(1혹은 -1)라고 했을 때 퍼셉트론의 손실함수는 **max(0, -twx)(힌지 손실)**\n",
    "- 퍼셉트론의 힌지 손실 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97896cd6-2475-47be-a2f2-bc3bb535e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def perceptron_hinge_loss(w, x, t):\n",
    "    loss = 0\n",
    "    \n",
    "    for (input, label) in zip(x, t):\n",
    "        v = label * np.dot(w, input)\n",
    "        loss += max(0, -v)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382498f7-b7c5-481c-964e-cebfb0b56eca",
   "metadata": {},
   "source": [
    "- 손실 함수를 일반화하여 목적 함수(평가 함수)로 만든다.\n",
    "> 퍼셉트론의 목적 함수: **목적 함수 = 모든 데이터에 대한 손실 함수값의 합**\n",
    "\n",
    "- 목적 함수가 최소가 되는(최적 상태) 가중치 벡터 w를 구하는 과정이 **'모델 학습'**\n",
    "- 파라미터(가중치 벡터 w) 최적화에는 **확률적 경사하강법<sup>stochastic gradient descent</sup>(SGD)**를 많이 사용\n",
    "- 파라미터를 한 번에 어느 정도씩 수정할지를 결정하는 하이퍼파라미터가 **학습률<sup>learning rate</sup>**\n",
    "- 출력 값을 비선형 변환하는 함수는 **활성화 함수<sup>activation function</sup>**\n",
    "\n",
    "### 2.2.2 로지스틱 회귀\n",
    "\n",
    "- 이름은 회귀지만 분류 알고리즘\n",
    "- 간단하면서도 파라미터가 많지 않아 빠르게 예측 가능\n",
    "\n",
    "---\n",
    "![로지스틱회귀](./images/logistic.png)\n",
    "\n",
    "#### **로지스틱 회귀의 특성**\n",
    "\n",
    "- 출력과 별도로, 출력 값에 해당하는 클래스에 속할 '확률'을 계산할 수 있다.\n",
    "- 온라인 학습과 배치 학습이 모두 가능하다.\n",
    "- 예측 성능은 보통이며 학습 속도가 빠르다.\n",
    "- 과적합을 방지하는 규제항이 추가되어 있다.\n",
    "\n",
    "#### **로지스틱 회귀의 결정 경계**\n",
    "\n",
    "- 결정 경계가 직선이다(선형 분리만 가능)\n",
    "\n",
    "#### **로지스틱 회귀의 구조**\n",
    "\n",
    "- 활성화 함수가 시그모이드 함수<sup>sigmoid function</sup>(혹은 로지스틱 시그모이드 함수<sup>logistic sigmoid function</sup>)\n",
    "- 손실 함수가 교차 엔트로피 오차 함수<sup>cross-entropy error function</sup>이다.\n",
    "- 규제항<sup>regularization term</sup>이 추가되어 과적합을 방지할 수 있다. \n",
    "- 시그모이드 함수 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "745600c4-6a9c-43e0-a514-b59ac3f2e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a42a1-0c87-454a-8030-6a46bdab6e63",
   "metadata": {},
   "source": [
    "- 출력 `y = sigmoid(np.dot(w, x))`로 나타낼 수 있다.\n",
    "- 교차 엔트로피 오차 함수의 식\n",
    "> $E = - \\displaystyle\\sum_{n=1}^{N}{t_n}\\log{y_n} + (1 - t_n)\\log{1-y_n}$  \n",
    "N: 데이터 수, y: 출력, t: 정답 레이블(정답은 1, 틀리면 0), 로그의 밑은 자연상수\n",
    "- 교차 엔트로피 함수 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5295377a-4cdb-48ae-8a83-547dd172af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t, eps=1e-15):\n",
    "    y_clipped = np.clip(y, eps, 1-eps)\n",
    "    return -1 * (sum(t * np.log(y_clipped) + (1-t) * np.log(1 - y_clipped)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2212a7-3e8b-4766-8aed-2ab05f2a204f",
   "metadata": {},
   "source": [
    "- 로그 값이 음의 무한대가 되는 일을 피하기 위해 y 값이 0 또는 1이 되지 않도록 eps라는 작은 값을 더해준다.\n",
    "- **규제화<sup>regularization</sup>**는 패널티를 부여하여 결정 경계를 매끄럽게 하며, 간단한 모델을 유지하도록 보정한다.\n",
    "- 결국 과적합을 방지하도록 한다.\n",
    "- 규제항이 포함된 목적 함수\n",
    "> 목적 함수 = 모든 데이터에 대한 손실 함수 값의 합 + 규제항\n",
    "- 확률적 경사하강법으로 최적화\n",
    "> L2 규제: 가중치의 제곱의 합을 규제항으로 사용. 큰 가중치에 페널티가 부여된다. 릿지 회귀에 사용  \n",
    "목적 함수 = 모든 데이터에 대한 손실 함수 값의 합 + $\\lambda\\displaystyle\\sum_{i=1}^{m}{w_i^2}$  \n",
    "L1 규제: 가중치의 절댓값의 합을 규제항으로 사용. 가중치가 0으로 제거되는 경우가 많아진다. 라쏘 회귀에 사용  \n",
    "목적 함수 = 모든 데이터에 대한 손실 함수 값의 합 + $\\lambda\\displaystyle\\sum_{i=1}^{m}\\vert{w_i}\\vert$  \n",
    "일래스틱넷은 두 규제를 모두 사용\n",
    "\n",
    "### 2.2.3 서포트 벡터 머신<sup>support vector machine</sup> (SVM)\n",
    "\n",
    "- 분류에서 매우 많이 사용\n",
    "- **선형 분리 불가능한 문제에도 적용 가능**\n",
    "- 다양한 알고리즘과 라이브러리 존재\n",
    "\n",
    "#### **SVM의 특성**\n",
    "- 마진 최대화를 통해 매끈한 초평면을 학습할 수 있다.\n",
    "- 커널<sup>kernel</sup>이라는 방법을 사용하여 비선형 데이터를 분리할 수 있다.\n",
    "- 선형 커널로는 차원 수가 높은 희소<sup>sparse</sup> 데이터도 학습할 수 있다.\n",
    "- 배치 학습과 온라인 학습에 모두 적용할 수 있다.\n",
    "\n",
    "#### **SVM의 결정 경계**\n",
    "\n",
    "- 선형 커널은 직선으로 분리, RBF 커널은 비선형으로 분리\n",
    "\n",
    "#### **SVM의 구조**\n",
    "\n",
    "- 손실 함수는 힌지 손실을 사용(퍼셉트론과는 가로축과의 교점이 달라 결정 경계에 마진이 생긴다).\n",
    "- 마진을 최대화하면서 규제화항과 비슷한 과적합 억제 효과를 얻는다.\n",
    "- 커널 기법 사용\n",
    "> 커널 기법 : 선형 분리 불가능한 데이터에 커널 함수를 적용하여 데이터의 차원 수를 늘려 선형 분리 가능한 형태로 바꾸는 방법  \n",
    "선형 커널<sup>linear kernel</sup>, 다항식 커널<sup>polynomial kernel</sup>, RBF 커널<sup>radial basis function kernel</sup>(동적 기저함수 커널) 등이 있다.  \n",
    "선형 커널은 속도가 빨라 텍스트 같은 고차원 희소 벡터(**입력 벡터의 요소 대부분이 0인 벡터**)에 사용  \n",
    "RBF 커널은 이미지나 음성신호 같은 조밀한 데이터에 주로 사용\n",
    "---\n",
    "![커널 기법](./images/kernel.png)\n",
    "\n",
    "### 2.2.4 신경망<sup>neural network</sup>\n",
    "\n",
    "- 다층 퍼셉트론이라고도 부름\n",
    "- 퍼셉트론을 하나의 노드로 하여 계층적으로 쌓아놓은 구조\n",
    "---\n",
    "![신경망](./images/nn.png)\n",
    "\n",
    "\n",
    "#### **신경망의 특성**\n",
    "\n",
    "- 비선형 데이터를 분리할 수 있다.\n",
    "- 학습 시간이 오래 걸린다.\n",
    "- 파라미터 수가 많으므로 과적합을 일으키기 쉽다.\n",
    "- 가중치의 초깃값에 민감하며, 지역 최적점<sup>local optima</sup>에 빠지기 쉽다.\n",
    "- 데이터가 많이 필요하다.\n",
    "\n",
    "#### **신경망의 결정 경게**\n",
    "\n",
    "- 직선이 아닌 결정 경계를 가질 수 있다.\n",
    "\n",
    "#### **신경망의 구조**\n",
    "\n",
    "- 입력층, 은닉층(중간층), 출력층 순서로 입력과 가중치의 곱셈합을 구한다.\n",
    "- 출력층에는 분류하려는 클래스의 수만큼 노드를 배치한다.\n",
    "- 출력층에서 계산한 값을 **softmax 함수**로 정규화한 값을 확률로 사용하는 경우가 많다.\n",
    "- 은닉층의 노드 수는 입력층과 출력층의 노드 수에 맞춰 적절히 설정\n",
    "- 활성화 함수는 **ReLU<sup>rectified linear unit</sup> 함수**가 많이 쓰인다.\n",
    "- 피드포워드 신셩망 학습에는 역전파<sup>backpropagation</sup> 방법을 쓴다.\n",
    "\n",
    "### 2.5.5 k-최근점 이웃<sup>k-nearest neighbor</sup>(k-NN)\n",
    "\n",
    "- k-최근접 이웃은 새로운 데이터의 클래스를 다수결로 정한다.\n",
    "- 진행 과정\n",
    "    - 이미 학습된 데이터 중 새로 입력된 데이터와 거리가 가장 가까운 k 개를 선택\n",
    "    - 이 k개 중 가장 많은 데이터가 속한 클래스를 찾음\n",
    "    - 새로운 데이터를 이 클래스로 분류\n",
    "\n",
    "#### **k-NN의 특성**\n",
    "\n",
    "- 데이터를 하나씩 순차적으로 학습한다.\n",
    "- 기본적으로 모든 데이터와의 거리를 게산해야 하므로 예측 시간이 걸린다.\n",
    "- k값에 따라 편차는 있지만 예측 성능은 괜찮은 편이다.\n",
    "- 스케일 차이가 클수록 성능이 떨어지므로 정규화<sup>nomalization</sup>로 스케일 조정 필요\n",
    "\n",
    "#### **k-NN의 결정 경계**\n",
    "\n",
    "- k값 설정에 따라 결정 경계가 좌우됨(k값이 커질수록 결정 경계는 매끈해지나 처리 시간이 길어진다)\n",
    "\n",
    "#### **k-NN의 구조**\n",
    "\n",
    "- k는 예측 클래스를 결정하는 총 투표 수\n",
    "- k값은 교차검증을 통해 결정한다.\n",
    "- 두 데이터 사이의 거리는 유클리드 거리<sup>Euclidean distance</sup>다.\n",
    "- 데이터가 분포하는 방향을 고려하는 마하라노비스 거리<sup>Mahalanobis distance</sup>을 사용하기도 한다.\n",
    "- 높은 차원의 희소 데이터에서는 성능이 떨어져 차원 축소를 통한 개선이 필요하다.\n",
    "- 유클리드 거리를 구하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78d17b9b-9e30-4786-925c-e9fdd2b44a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(sum(x-y) ** 2 for (x, y) in zip(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f15767-1f4d-409c-be66-1ca9eae4c632",
   "metadata": {},
   "source": [
    "### 2.2.6 결정 트리, 랜덤 포레스트, GBDT\n",
    "\n",
    "- 결정트리는 트리 형태 알고리즘의 대표 주자\n",
    "---\n",
    "![결정 트리](./images/tree.png)\n",
    "\n",
    "#### **결정 트리의 특성**\n",
    "\n",
    "- 학습한 모델을 사람이 해석하기 쉽다.\n",
    "- 입력 데이터를 정규화할 필요가 없다.\n",
    "- 범주형 변수나 데이터의 누락값이 있어도 용인된다.\n",
    "- 특정 조건이 맞으면 과적합을 일으키는 경향이 있다(트리가 깊어질수록 또는 특징 수가 많을수록 과적합이 일어나기 쉽다).\n",
    "- 비선형 문제에는 적용할 수 있지만 선형 분리 문제는 잘 풀지 못한다.\n",
    "- 데이터 분포가 특정 클래스에 쏠려 있으면 잘 풀지 못한다.\n",
    "- 데이터의 작은 변화에도 결과가 크게 바귀기 쉽다.\n",
    "- 예측 성능은 보통이다.\n",
    "- 배치 학습으로만 학습할 수 있다.\n",
    "\n",
    "> 특정한 분류 결과에 이르게 되는 조건을 알아야 할 때 유용하다.  \n",
    "과적합 문제는 트리의 깊이를 줄이는 가지치기<sup>pruning</sup>으로 어느 정도 방지 가능하다.\n",
    "\n",
    "#### **결정 트리의 결정 경계**\n",
    "\n",
    "- 영역을 반복해서 나눠가는 과정에서 정해지기 때문에 직선 형태를 취하지 않는다.\n",
    "\n",
    "#### **결정 트리의 구조**\n",
    "\n",
    "- 학습 데이터로부터 조건식을 만들고, 예측할 때는 트리의 루트<sup>root node</sup>부터 순서대로 조건 분기를 타면서 리프<sup>leaf node</sup>에 도달하면 예측 결과를 낸다.\n",
    "- **불순도**<sup>impurity</sup>를 기준으로 가능한 한 같은 클래스끼리 모이도록 조건 분기를 학습한다.\n",
    "- 정보 획득<sup>information gain</sup>이나 지니 계수<sup>Gini coefficient</sup> 등의 값을 불순도로 사용한다.\n",
    "\n",
    "#### **결정 트리로부터 파생된 알고리즘**\n",
    "\n",
    "- 랜덤 포레스트<sup>random forest</sup>\n",
    "    - 몇 개의 특징 조합을 가지고 성능이 좋았던 여러 개의 학습기가 내린 예측 결과를 다수결로 통합\n",
    "    - 각각의 트리는 독립적으로 학습(병렬 학습 가능)\n",
    "    - 가지치기가 없고 주된 파라미터가 2개로 과적합을 일으키기 쉽다.\n",
    "    - 결정 트리보다 성능이 좋고 파라미터 수가 적어 튜닝도 비교적 간단하다.\n",
    "    - 결정 경계는 결정 트리와 비슷하다.\n",
    "- GBDT<sup>gradient boosted decision tree</sup>\n",
    "    - 표본추출한 데이터를 이용해 순차적으로 얕은 트리를 학습해가는 경사 부스팅<sup>gradient boosting</sup>을 사용\n",
    "    - 예측값과 실젯값의 오차를 목표 변수로 삼음\n",
    "    - 여러 개의 학습기로 학습\n",
    "    - 순차적 학습으로 시간이 오래 걸림\n",
    "    - 파라미터 수가 많아 튜닝이 쉽지 않음\n",
    "    - 랜덤 포레스트보다 더 뛰어난 예측 성능\n",
    "    - XGBoost나 LightGBM 등의 라이브러리를 통해 대규모 데이터도 쉽고 빠르게 처리 가능\n",
    "    \n",
    "> 랜덤 포레스트나 GBDT처럼 여러 학습 결과를 조합하는 기법을 **앙상블 학습**<sup>ensemble learning</sup>이라 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e067ed29-0f65-4a02-a3b8-20b1ca76071f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a451d890-74fa-4cd4-99c1-59b3bc5830eb",
   "metadata": {},
   "source": [
    "# 2.3 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af7b795-18ec-415f-9227-4695264246a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
